# 卷积

## 卷积函数
卷积的参数一般包含了

	输出输出通道数：c1和c2
	卷积核大小：k
	步长：S
	填充：P
	
输出尺寸和输入尺寸的关系是：
	
	Hout=|_Hin+2p-k/s_| +1
	
	
	
	
	

## RepConv
重参数卷积


https://github.com/WongKinYiu/yolov7/blob/main/models/common.py

https://github.com/ultralytics/yolov5/blob/master/models/common.py#L210

https://github.com/ultralytics/ultralytics/blob/main/ultralytics/nn/modules/conv.py

# 下采样
下采样（Downsampling）是指通过减少数据量或分辨率来简化信息的处理技术，其具体定义和应用场景因领域而异






# 归一化
L1 归一化 (L1 Normalization)​​ 和 ​​L2 归一化 (L2 Normalization)​​ ：将向量​​缩放到一个特定的范围或范数，过某种数学变换，将原始向量的范数缩放为 1 (单位范数)​​。


## L1 归一化
曼哈顿范数, Taxicab范数

方法：将向量中的​​每一个元素​​除以该向量的 L1 范数。
x_normalized​​ = ​​x​​ / ||​​x​​||₁ = [x₁ / (|x₁| + |x₂| + ... + |xn|), x₂ / (|x₁| + |x₂| + ... + |xn|), ..., xn / (|x₁| + |x₂| + ... + |xn|)]


特点：
1. 归一化后，向量的所有元素都是​​非负​​的（如果原始向量包含负数，其绝对值会参与范数计算，但元素本身除以正数后符号保留，所以结果向量可能包含负数）

2. 归一化后向量所有元素的绝对值之和恒等于 1​​。

3. 常用于文本处理（如词袋模型表示文档）、图像直方图处理、需要概率解释的场景。

4. 对​​异常值 (Outliers)​​ 相对不敏感一些。异常值会被较大的范数“稀释”，因为分母比较大


## L2 归一化
Unit L2 Norm, Euclidean Normalization

​​方法：​​ 将向量中的​​每一个元素​​除以该向量的 L2 范数。
x_normalized​​ = ​​x​​ / ||​​x​​||₂ = [x₁ / √(x₁² + x₂² + ... + xn²), x₂ / √(x₁² + x₂² + ... + xn²), ..., xn / √(x₁² + x₂² + ... + xn²)]


特点：
1. ​​归一化后向量位于一个“单位球” (Unit Sphere) 的表面上。​

2. 余弦相似度计算的基础：​​ 两个 L2 归一化后的向量之间的点积 (**a** · **b**) 就等于它们夹角的余弦值 (cos(θ))。

3. ​​机器学习：​​ 在训练模型（特别是神经网络）时，常对特征向量进行 L2 归一化或对权重参数进行 L2 归一化/约束，有助于提高优化过程的数值稳定性（如梯度下降）

4. ​ L2 归一化​​保留了原始向量的方向​​（即向量之间的夹角），只改变了向量的大小（长度）。这对于许多关注角度或相似性的任务至关重要。



# 损失函数
量化模型预测结果与真实值之间的差距，作为衡量模型性能的核心指标。

## 分类问题中
### 0-1损失函数
预测结果与真实标签一致，则损失为0；若不一致，则损失为1。

### 交叉熵损失（Cross-Entropy Loss）
衡量两个概率分布之间的差异。在分类任务中，真实标签是one-hot编码的确定分布（如类别3的概率为1，其余为0），而模型输出是预测的概率分布。交叉熵通过计算两者之间的信息差异，指导模型调整预测概率逼近真实分布。
L = sum(- y_c*log(y~_c))
y_c是真是标签的概率0 或1
y~_c 是经过softmax归一化后的类别c的概率

优点：梯度更新方向明确，收敛速度快。
缺点：对类别不平衡敏感，需结合加权或采样策略。



### 合页损失（Hinge Loss）
最大化分类边界（Margin），要求正确类别的得分比其他类别至少高出一个固定边界值（通常为1）

仅能对样本进行正确分类，而且还要让分类决策具有​​最大化间隔（Margin）​​ 的模型。它特别适用于二分类问题

L_i = max(0, 1 - y_i * f(x_i))

优点：生成清晰的分类边界，对噪声鲁棒。
缺点：不直接输出概率，需后处理（如Platt Scaling）。




### ArcFace Loss (Additive Angular Margin Loss)
学习一个 ​​高度判别性的特征嵌入空间​​。
核心在于​​减小类内（同一个人的不同样本）特征距离，增大类间（不同人）特征距离​​。通过在​​角度空间​​施加一个​​强约束的间隔​​，迫使同类别样本在嵌入空间中聚集在更紧凑的超球面区域，不同类别的中心点彼此分离得更远。




## 回归问题

### 均方误差（Mean Squared Error, MSE）
就是两者差的平方和求平均

优点：数学性质良好（处处可导），梯度计算高效，适合梯度下降优化。
缺点：对离群点敏感（平方放大误差），可能导致模型过度拟合异常值。


### 平均绝对误差（Mean Absolute Error, MAE）
两个差不是平方而是绝对值。

优点：对离群点鲁棒（线性惩罚），梯度稳定。
缺点：在零点处不可导，收敛速度可能较慢。
适用场景：存在明显离群点的数据（如传感器噪声数据）。






# 文本编码

## one-hot编码
将分类变量（特别是文本中的词/标记）转换为数值形式​​的方法
方法：
1. 对于词汇表中的 ​​每一个词​​，都创建一个长度等于词汇表大小的​​向量​​。
2. 在这个向量中，​​只有该词在词汇表中对应位置的元素是 1（“热”）​​。
​​3. 向量中所有其他位置的元素都是 0（“冷”）​​。


特点：
1. 维度等于词汇表大小（V）
2. ​​词间无语义信息:​​ 向量中的维度只是索引编号，​​没有表达任何词本身的含义或词与词之间的关系​​。
3. 这种方法于L2归一相结合可以，将所有文本表示拉到一个单位超球面上，确保不同长度文本的向量具有可比性。两个L2归一化后的（基于One-Hot/BOW/Multi-Hot）向量之间的点积直接等于它们的余弦相似度（cosθ），这个相似度只关心向量夹角（词语的共现情况），而不关心向量原始长度（文本绝对长度）。


## 词嵌入 (Word Embeddings)

### Word2Vec
核心思想：“通过上下文学习词义”​​，基于分布假说：​​出现在相似上下文中的词具有相似语义​​。

​​1. 输入层​​：词语转为 one-hot 向量（维度 = 词汇表大小 V）。
​​2. 隐藏层​​：无激活函数的全连接层，生成词向量（维度 N，如 300 维）。
​​3. 输出层​​：Softmax 预测目标词概率






## CLIP（Contrastive Language-Image Pre-training）

核心目标是将图像和文本映射到统一的语义空间中，实现跨模态的理解与匹配

方法：对比学习与双塔架构，
双编码器结构​​：
1. ​​图像编码器​​：采用 Vision Transformer（ViT）或 ResNet，将图像转换为特征向量（例如 [0.82, -0.45, ...]）。
​​2. 文本编码器​​：基于 Transformer（如 BERT），将文本描述转换为特征向量。
​​共享向量空间​​：两种模态的特征向量被映射到同一空间，相似图文对的向量距离接近，不相似则远离。



# 支持向量机SVM
是一种经典的监督学习算法，主要用于​​二元分类​​，也可扩展至回归、聚类和多分类任务。
其核心思想是​​寻找最大化分类间隔的最优超平面​​

找到一个超平面 w⋅x+b=0，将两类样本分开，并最大化支持向量（距离超平面最近的样本点）到超平面的距离（间隔）。
间隔计算公式为 2/∥w∥，优化问题转化为最小化 1/2∥w∥^2
 
 


 

